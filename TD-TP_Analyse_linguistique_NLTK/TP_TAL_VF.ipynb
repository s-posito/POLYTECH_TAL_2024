{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP1 de TAL (Polytech Paris-Saclay 2024 - Traitement Automatique des Langues)\n",
        "\n"
      ],
      "metadata": {
        "id": "fEZUrYR_7GMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SzD3ReK5bHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf37fb2-856d-492f-d629-dc63c0dfc4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --user -U numpy"
      ],
      "metadata": {
        "id": "UyFclxOk6_qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a551c2-16e6-4ca5-b88f-634595ccf972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[33m  WARNING: The script f2py is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test de NLTK (pour voir si tout fonctionne)"
      ],
      "metadata": {
        "id": "c1HFtEc07UWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZSzPFxFPqzdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c95a35-b7a7-416d-ffe8-8d572436062a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"It’s works!\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqejiIQm7FNX",
        "outputId": "444e5523-e0f5-45e3-f1e0-3f58453cce12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', '’', 's', 'works', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Evaluation de l’analyse morpho-syntaxique de la plateforme NLTK"
      ],
      "metadata": {
        "id": "FyaqYa078M5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6YxG4178Wnc",
        "outputId": "4610eb2f-03ba-4fa7-bd21-c29bc5ba385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ecrire un programme Python utilisant le package pos_tag pour désambiguïser morpho-\n",
        "syntaxiquement le texte du fichier wsj_0010_sample.txt. Le résultat de ce module sera\n",
        "mis dans le fichier wsj_0010_sample.txt.pos.nltk.\n",
        "\n",
        "Note :\n",
        "- Un exemple d’utilisation du package pos_tag se trouve sur le lien\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html.\n",
        "- L’analyseur morpho-syntaxique de la plateforme NLTK utilise les étiquettes du PennTreeBank.\n",
        "\n",
        "Format du résultat de l’analyse morpho-syntaxique:\n",
        "Le fichier wsj_0010_sample.txt.pos.nltk doit avoir le format suivant :\n",
        "\n",
        "Token \\t Tag (\\t correspond à tabulation)\n",
        "\n",
        "Exemple :\n",
        "\n",
        "When WRB\n",
        "\n",
        "it PRP\n",
        "\n",
        "'s VBZ\n",
        "\n",
        "time NN\n",
        "\n",
        "for IN\n",
        "\n",
        "their PRP$\n",
        "\n",
        "biannual JJ\n",
        "\n",
        "powwow NN\n",
        "\n",
        ", ,\n",
        "…"
      ],
      "metadata": {
        "id": "3JakVEFr4r58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "output_filename = 'wsj_0010_sample.txt.pos.nltk'\n",
        "with open(output_filename, 'w') as output_file:\n",
        "    output_file.write(tagged_text)\n",
        "\n",
        "print(\"Résultats écrits dans\", output_filename, \"- Fichier généré\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2LUH1l8Mas",
        "outputId": "b391a122-014b-45b0-a159-89a67ab28187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.pos.nltk - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ecrire un programme Python permettant de convertir les étiquettes Penn TreeBank du fichier\n",
        "wsj_0010_sample.txt.pos.nltk en étiquettes universelles en utilisant la table de\n",
        "correspondance POSTags_PTB_Universal_Linux.txt. Le fichier résultat sera nommé\n",
        "wsj_0010_sample.txt.pos.univ"
      ],
      "metadata": {
        "id": "WxBXOkPA4gCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wsj_0010_sample.txt.pos.nltk', 'r') as ptb_file:\n",
        "    ptb_tags = ptb_file.readlines()\n",
        "\n",
        "universal_tags = []\n",
        "for ptb_line in ptb_tags:\n",
        "    word, ptb_tag = ptb_line.strip().split('\\t')\n",
        "    universal_tags.append(f'{word}\\t{ptb_tag}')\n",
        "\n",
        "with open('wsj_0010_sample.txt.pos.univ', 'w') as output_file:\n",
        "    output_file.write('\\n'.join(universal_tags))\n",
        "\n",
        "print(\"Résultats écrits dans wsj_0010_sample.txt.pos.univ - Fichier généré\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYzhU4aU4frz",
        "outputId": "4918183f-d2b1-4227-c849-360dd57f3ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.pos.univ - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ecrire un programme Python permettant de construire à partir du fichier annoté au format CONLL wsj_0010_sample.txt.conll un fichier contenant uniquement les phrases à annotées. Le fichier résultat sera nommé wsj_0010_sample.txt.\n",
        "\n",
        "**Format du fichier contenant uniquement les phrases à annotées:**\n",
        "\n",
        "Le fichier wsj_0010_sample.txt doit avoir le format suivant :\n",
        "\n",
        "Phrase 1\n",
        "\n",
        "Phrase 2\n",
        "\n",
        "…\n",
        "\n",
        "**Exemple :**\n",
        "\n",
        "When it's time for their biannual powwow, the nation's manufacturing titans typically jet off to the sunny confines of resort towns like Boca Raton and Hot Springs.\n",
        "\n",
        "Not this year."
      ],
      "metadata": {
        "id": "0etY34fjnXFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'wsj_0010_sample.txt.conll'\n",
        "output_file_path = 'wsj_0010_sample.txt'\n",
        "\n",
        "sentences = []\n",
        "current_sentence = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            word = line.split('\\t')[1]\n",
        "            current_sentence.append(word)\n",
        "        else:\n",
        "            if current_sentence:\n",
        "                sentences.append(' '.join(current_sentence))\n",
        "                current_sentence = []\n",
        "\n",
        "if current_sentence:\n",
        "    sentences.append(' '.join(current_sentence))\n",
        "\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for sentence in sentences:\n",
        "        output_file.write(sentence + '\\n')\n",
        "\n",
        "output_file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_r4EnQXnn4Sz",
        "outputId": "96657d77-02bd-4d8f-fdac-3675930a1d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wsj_0010_sample.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ecrire un programme Python permettant de construire à partir du fichier annoté au format CONLL *wsj_0010_sample.txt.conll* un fichier contenant uniquement les annotations en POS tags. Le fichier résultat sera nommé *wsj_0010_sample.txt.pos*"
      ],
      "metadata": {
        "id": "-ooYi8aGy5gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'wsj_0010_sample.txt.conll'\n",
        "output_file_path_pos = 'wsj_0010_sample.txt.pos'\n",
        "\n",
        "with open(file_path, 'r') as input_file, open(output_file_path_pos, 'w') as output_file:\n",
        "    for line in input_file:\n",
        "        if line.strip():\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) > 3:\n",
        "                token = parts[1]\n",
        "                tag = parts[3]\n",
        "                output_file.write(f'{token}\\t{tag}\\n')\n"
      ],
      "metadata": {
        "id": "gUT3omIly5u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Utilisation de la plateforme NLTK pour l’analyse syntaxique"
      ],
      "metadata": {
        "id": "7YVhQMJ38-VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Ecrire un programme Python utilisant le package parse pour extraire les mots composés\n",
        "(chunks) ayant la structure syntaxique Déterminant-Adjectif-Nom (grammar =\n",
        "\"Compound: \\{\\<DT\\>?\\<JJ\\>*\\<NN\\>\\}\") présents dans le texte du fichier\n",
        "wsj_0010_sample.txt. Le résultat de ce module sera mis dans le fichier\n",
        "wsj_0010_sample.txt.chk.nltk.\n",
        "Note :\n",
        "Un exemple d’utilisation du package parse se trouve sur le lien\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html."
      ],
      "metadata": {
        "id": "DR7DK9JR6hpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "O0B7vDrH99Lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c815469-cc53-43c8-f2af-480e54b262fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "with open('tags_temp.txt', 'w') as tags_temp_file:\n",
        "    tags_temp_file.write(tagged_text)\n",
        "\n",
        "grammar = r\"\"\"\n",
        "    Compound: {<DT>?<JJ>*<NN>}\n",
        "\"\"\"\n",
        "\n",
        "cp = RegexpParser(grammar)\n",
        "\n",
        "chunked = cp.parse(tagged_tokens)\n",
        "\n",
        "output_chunks = []\n",
        "\n",
        "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Compound'):\n",
        "    chunk_words = [word for word, tag in subtree.leaves()]\n",
        "\n",
        "    if any(word[1].startswith('DT') for word in subtree.leaves()) and \\\n",
        "       any(word[1].startswith('JJ') for word in subtree.leaves()) and \\\n",
        "       any(word[1].startswith('NN') for word in subtree.leaves()):\n",
        "        output_chunks.append(' '.join(chunk_words))\n",
        "\n",
        "output_text = \"\\n\".join(output_chunks)\n",
        "\n",
        "with open('wsj_0010_sample.txt.chk.nltk', 'w') as output_file:\n",
        "    output_file.write(output_text)\n",
        "\n",
        "print(f\"Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\")\n"
      ],
      "metadata": {
        "id": "OwWT-OFf9ASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb339d53-a9e9-476d-d666-4cc07087a5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Généraliser le programme Python précédent pour extraire les mots composés (chunks) compatibles avec les structures syntaxiques ci-dessous :"
      ],
      "metadata": {
        "id": "j1B_QsJQ2tPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Adjectif-Nom\n",
        "\n",
        "Nom-Nom\n",
        "\n",
        "Adjectif-Nom-Nom\n",
        "\n",
        "Adjectif-Adjectif-Nom\n",
        "\n",
        "*Note : Il est recommandé d’utiliser un fichier déclaratif contenant ces structures syntaxiques.*\n",
        "\n",
        "\n",
        "**Format du résultat de l’analyse syntaxique:**\n",
        "\n",
        "Le fichier wsj_0010_sample.txt.chk.nltk doit avoir le format suivant :\n",
        "\n",
        "Pattern:\n",
        "\n",
        "    Mot composé\n",
        "\n",
        "Exemple :\n",
        "\n",
        "**Adjectif-Nom:**\n",
        "\n",
        "    it's time\n",
        "    nation's manufacturing\n",
        "    corporate decision\n",
        "    good place\n",
        "\n",
        "**Nom-Nom:**\n",
        "\n",
        "    fall board\n",
        "    …"
      ],
      "metadata": {
        "id": "Qh9z-1ldB5jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "with open('tags_temp.txt', 'w') as tags_temp_file:\n",
        "    tags_temp_file.write(tagged_text)\n",
        "\n",
        "grammars = {\n",
        "    'Adjectif-Nom': r\"AN: {<JJ><NN|NNS>}\",\n",
        "    'Nom-Nom': r\"NN: {<NN><NN|NNS>}\",\n",
        "    'Adjectif-Nom-Nom': r\"ANN: {<JJ><NN|NNS><NN|NNS>}\",\n",
        "    'Adjectif-Adjectif-Nom': r\"AAN: {<JJ><JJ><NN|NNS>}\"\n",
        "}\n",
        "\n",
        "output = {}\n",
        "\n",
        "for key, grammar in grammars.items():\n",
        "    cp = RegexpParser(grammar)\n",
        "    chunked = cp.parse(tagged_tokens)\n",
        "\n",
        "    found_chunks = []\n",
        "    for subtree in chunked.subtrees():\n",
        "        if subtree.label() in ['AN', 'NN', 'ANN', 'AAN']:\n",
        "            chunk = \" \".join(word for word, tag in subtree.leaves())\n",
        "            found_chunks.append(chunk)\n",
        "\n",
        "    output[key] = found_chunks\n",
        "\n",
        "output_text = \"\"\n",
        "for key in grammars:\n",
        "    output_text += f\"{key}:\\n\" + \"\\n\".join(output[key]) + \"\\n\\n\"\n",
        "\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfwhAI5WBLeW",
        "outputId": "ae4f6c92-9a3d-4cdc-8d73-605d292c2a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjectif-Nom:\n",
            "biannual powwow\n",
            "sunny confines\n",
            "corporate decision\n",
            "good place\n",
            "corporate manufacturing\n",
            "\n",
            "Nom-Nom:\n",
            "manufacturing titans\n",
            "resort towns\n",
            "board meeting\n",
            "rock stars\n",
            "factory owners\n",
            "decision makers\n",
            "\n",
            "Adjectif-Nom-Nom:\n",
            "corporate decision makers\n",
            "\n",
            "Adjectif-Adjectif-Nom:\n",
            "biannual corporate manufacturing\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Utilisation de la plateforme NLTK pour l’extraction d’entités nommées"
      ],
      "metadata": {
        "id": "FqgXpw07CJDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Ecrire un programme Python utilisant le package ne_chunk pour extraire les entités nommées présentes dans le texte du fichier wsj_0010_sample.txt. Le résultat de ce module sera mis dans le fichier wsj_0010_sample.txt.ne.nltk.\n"
      ],
      "metadata": {
        "id": "RhWacvy4CS1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : Un exemple d’utilisation du package ne_chunk se trouve sur le lien\n",
        "https://pythonprogramming.net/named-entity-recognition-nltk-tutorial/.\n",
        "\n",
        "**Format du résultat de l’extraction d’entités nommées:**\n",
        "\n",
        "Le fichier wsj_0010_sample.txt.ne.nltk doit avoir le format suivant :\n",
        "\n",
        "    Token \\t Tag (\\t correspond à tabulation)\n",
        "\n",
        "Exemple :\n",
        "\n",
        "    Boca Raton PERSON\n",
        "    Hot Springs PERSON\n",
        "    National Association ORGANIZATION\n",
        "    Hoosier ORGANIZATION\n",
        "    Indianapolis GPE\n",
        "    Rust Belt ORGANIZATION"
      ],
      "metadata": {
        "id": "bucV7So_CegA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JU7tKtCDrqx",
        "outputId": "78da6bfb-3c50-4109-8635-c363e0929481"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "with open('formal-tst.NE.key.04oct95_small.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "entities = ne_chunk(tags)\n",
        "named_entities = []\n",
        "\n",
        "for entity in entities:\n",
        "    if isinstance(entity, Tree):\n",
        "        et = \" \".join([word for word, tag in entity.leaves()])\n",
        "        label = entity.label()\n",
        "        named_entities.append((et, label))\n",
        "\n",
        "with open('wsj_0010_sample.txt.ne.nltk', 'w') as output_file:\n",
        "    for ne in named_entities:\n",
        "        output_file.write(f\"{ne[0]}\\t{ne[1]}\\n\")\n",
        "\n",
        "print(\"Extraction d'entités nommées terminée et enregistrée dans wsj_0010_sample.txt.ne.nltk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_htxLhL8CH_2",
        "outputId": "3781b23b-f1eb-4cd7-a51a-8b5208ca5644"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction d'entités nommées terminée et enregistrée dans wsj_0010_sample.txt.ne.nltk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Ecrire un programme Python permettant de convertir les étiquettes NLTK du fichier wsj_0010_sample.txt.ne.nltk en étiquettes universelles en utilisant la table de correspondance ERTags_NLTK_Universal_Linux.txt. Le fichier résultat sera nommé wsj_0010_sample.txt.ne.univ."
      ],
      "metadata": {
        "id": "kvPHMfTkEKIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tag_mapping = {}\n",
        "with open('NERTags_PTB_Universal_Linux.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            ptb_tag, universal_tag = line.strip().split()\n",
        "            tag_mapping[ptb_tag] = universal_tag\n",
        "\n",
        "converted_tags = []\n",
        "with open('wsj_0010_sample.txt.ne.nltk', 'r') as file:\n",
        "    for line in file:\n",
        "        entity, nltk_tag = line.strip().split('\\t')\n",
        "        universal_tag = tag_mapping.get(nltk_tag, 'O')\n",
        "        converted_tags.append(f\"{entity}\\t{universal_tag}\\n\")\n",
        "\n",
        "with open('wsj_0010_sample.txt.ne.univ', 'w') as file:\n",
        "    file.writelines(converted_tags)\n",
        "\n",
        "print(\"Conversion terminée et enregistrée dans wsj_0010_sample.txt.ne.univ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UK4pt6EEUI8",
        "outputId": "d23a6bc9-3904-4bc9-8853-b987688ac34b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion terminée et enregistrée dans wsj_0010_sample.txt.ne.univ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. A partir du résultat de l’outil de reconnaissance des entités nommées wsj_0010_sample.txt.ne.univ, écrire un programme Python permettant de représenter les entités nommées sous un format tabulé XLS ou CSV (4 colonnes). Le fichier résultat sera nommé wsj_0010_sample.txt.ne.xls."
      ],
      "metadata": {
        "id": "Dy2ORAFmH-Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "input_file_path = 'wsj_0010_sample.txt.ne.univ'\n",
        "\n",
        "with open(input_file_path, 'r') as file:\n",
        "    data = file.readlines()\n",
        "\n",
        "entities = [line.strip().split('\\t') for line in data if line.strip()]\n",
        "entity_counter = Counter((entity, type_) for entity, type_ in entities)\n",
        "\n",
        "total_count = sum(entity_counter.values())\n",
        "\n",
        "entities_data = []\n",
        "\n",
        "for (entity, type_), count in entity_counter.items():\n",
        "    proportion = (count / total_count) * 100\n",
        "    entities_data.append([entity, type_, count, proportion])\n",
        "\n",
        "df_entities = pd.DataFrame(entities_data, columns=['Entity', 'Type', 'Count', 'Proportion'])\n",
        "\n",
        "df_entities.sort_values('Count', ascending=False, inplace=True)\n",
        "\n",
        "output_file_path = 'wsj_0010_sample.txt.ne.xls'\n",
        "df_entities.to_csv(output_file_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"File saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RfvJvHZIdx3",
        "outputId": "6e64c777-bbe6-45b9-f5c4-d899394c9b90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to wsj_0010_sample.txt.ne.xls\n"
          ]
        }
      ]
    }
  ]
}