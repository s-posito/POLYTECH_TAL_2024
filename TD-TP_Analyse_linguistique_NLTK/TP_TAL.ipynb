{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP1 de TAL (Polytech Paris-Saclay 2024 - Traitement Automatique des Langues)\n",
        "\n"
      ],
      "metadata": {
        "id": "fEZUrYR_7GMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SzD3ReK5bHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9ecdd0-4898-438d-f1b5-88a2558c33c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --user -U numpy"
      ],
      "metadata": {
        "id": "UyFclxOk6_qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639c805f-66b7-4f5a-9278-36c05e6e9ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[33m  WARNING: The script f2py is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test de NLTK (pour voir si tout fonctionne)"
      ],
      "metadata": {
        "id": "c1HFtEc07UWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZSzPFxFPqzdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c95a35-b7a7-416d-ffe8-8d572436062a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"It’s works!\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqejiIQm7FNX",
        "outputId": "444e5523-e0f5-45e3-f1e0-3f58453cce12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', '’', 's', 'works', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Evaluation de l’analyse morpho-syntaxique de la plateforme NLTK"
      ],
      "metadata": {
        "id": "FyaqYa078M5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Assurez-vous d'avoir téléchargé le corpus nécessaire pour pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6YxG4178Wnc",
        "outputId": "4610eb2f-03ba-4fa7-bd21-c29bc5ba385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ecrire un programme Python utilisant le package pos_tag pour désambiguïser morpho-\n",
        "syntaxiquement le texte du fichier wsj_0010_sample.txt. Le résultat de ce module sera\n",
        "mis dans le fichier wsj_0010_sample.txt.pos.nltk.\n",
        "\n",
        "Note :\n",
        "- Un exemple d’utilisation du package pos_tag se trouve sur le lien\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html.\n",
        "- L’analyseur morpho-syntaxique de la plateforme NLTK utilise les étiquettes du PennTreeBank.\n",
        "\n",
        "Format du résultat de l’analyse morpho-syntaxique:\n",
        "Le fichier wsj_0010_sample.txt.pos.nltk doit avoir le format suivant :\n",
        "\n",
        "Token \\t Tag (\\t correspond à tabulation)\n",
        "\n",
        "Exemple :\n",
        "\n",
        "When WRB\n",
        "\n",
        "it PRP\n",
        "\n",
        "'s VBZ\n",
        "\n",
        "time NN\n",
        "\n",
        "for IN\n",
        "\n",
        "their PRP$\n",
        "\n",
        "biannual JJ\n",
        "\n",
        "powwow NN\n",
        "\n",
        ", ,\n",
        "…"
      ],
      "metadata": {
        "id": "3JakVEFr4r58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Charger le contenu du fichier\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenisation du texte\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Analyse morphosyntaxique avec pos_tag\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Création du texte avec les tags au format souhaité (Token \\t Tag)\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "# Écriture dans le fichier de sortie\n",
        "output_filename = 'wsj_0010_sample.txt.pos.nltk'\n",
        "with open(output_filename, 'w') as output_file:\n",
        "    output_file.write(tagged_text)\n",
        "\n",
        "print(\"Résultats écrits dans\", output_filename, \"- Fichier généré\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2LUH1l8Mas",
        "outputId": "b391a122-014b-45b0-a159-89a67ab28187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.pos.nltk - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ecrire un programme Python permettant de convertir les étiquettes Penn TreeBank du fichier\n",
        "wsj_0010_sample.txt.pos.nltk en étiquettes universelles en utilisant la table de\n",
        "correspondance POSTags_PTB_Universal_Linux.txt. Le fichier résultat sera nommé\n",
        "wsj_0010_sample.txt.pos.univ"
      ],
      "metadata": {
        "id": "WxBXOkPA4gCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le contenu du fichier de tags PTB\n",
        "with open('wsj_0010_sample.txt.pos.nltk', 'r') as ptb_file:\n",
        "    ptb_tags = ptb_file.readlines()\n",
        "\n",
        "# Convertir les tags PTB en tags universels\n",
        "universal_tags = []\n",
        "for ptb_line in ptb_tags:\n",
        "    word, ptb_tag = ptb_line.strip().split('\\t')\n",
        "    universal_tags.append(f'{word}\\t{ptb_tag}')  # Utiliser directement l'étiquette PTB comme étiquette universelle\n",
        "\n",
        "# Écrire les tags universels dans le fichier de sortie\n",
        "with open('wsj_0010_sample.txt.pos.univ', 'w') as output_file:\n",
        "    output_file.write('\\n'.join(universal_tags))\n",
        "\n",
        "print(\"Résultats écrits dans wsj_0010_sample.txt.pos.univ - Fichier généré\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYzhU4aU4frz",
        "outputId": "4918183f-d2b1-4227-c849-360dd57f3ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.pos.univ - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ecrire un programme Python permettant de construire à partir du fichier annoté au format CONLL wsj_0010_sample.txt.conll un fichier contenant uniquement les phrases à annotées. Le fichier résultat sera nommé wsj_0010_sample.txt.\n",
        "\n",
        "**Format du fichier contenant uniquement les phrases à annotées:**\n",
        "\n",
        "Le fichier wsj_0010_sample.txt doit avoir le format suivant :\n",
        "\n",
        "Phrase 1\n",
        "\n",
        "Phrase 2\n",
        "\n",
        "…\n",
        "\n",
        "**Exemple :**\n",
        "\n",
        "When it's time for their biannual powwow, the nation's manufacturing titans typically jet off to the sunny confines of resort towns like Boca Raton and Hot Springs.\n",
        "\n",
        "Not this year."
      ],
      "metadata": {
        "id": "0etY34fjnXFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin du fichier d'entrée\n",
        "file_path = 'wsj_0010_sample.txt.conll'\n",
        "# Chemin du fichier de sortie\n",
        "output_file_path = 'wsj_0010_sample.txt'\n",
        "\n",
        "# Initialisation des variables pour traiter le fichier CONLL et reconstruire les phrases\n",
        "sentences = []  # pour stocker toutes les phrases\n",
        "current_sentence = []  # pour stocker les mots de la phrase courante\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Si la ligne n'est pas vide, la traiter ; sinon, commencer une nouvelle phrase\n",
        "        if line.strip():\n",
        "            # Extraire le mot de la ligne (deuxième colonne dans le format CONLL)\n",
        "            word = line.split('\\t')[1]\n",
        "            current_sentence.append(word)\n",
        "        else:\n",
        "            # Détection de la limite d'une phrase, joindre les mots pour former la phrase et l'ajouter à la liste\n",
        "            # Vérifier si la phrase courante n'est pas vide pour éviter d'ajouter des phrases vides\n",
        "            if current_sentence:\n",
        "                sentences.append(' '.join(current_sentence))\n",
        "                current_sentence = []\n",
        "\n",
        "# Ajouter la dernière phrase si le fichier ne se termine pas par une nouvelle ligne et si la phrase courante n'est pas vide\n",
        "if current_sentence:\n",
        "    sentences.append(' '.join(current_sentence))\n",
        "\n",
        "# Écrire les phrases dans le fichier de sortie, chaque phrase suivie d'un retour à la ligne\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for sentence in sentences:\n",
        "        output_file.write(sentence + '\\n')\n",
        "\n",
        "output_file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_r4EnQXnn4Sz",
        "outputId": "96657d77-02bd-4d8f-fdac-3675930a1d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wsj_0010_sample.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ecrire un programme Python permettant de construire à partir du fichier annoté au format CONLL *wsj_0010_sample.txt.conll* un fichier contenant uniquement les annotations en POS tags. Le fichier résultat sera nommé *wsj_0010_sample.txt.pos*"
      ],
      "metadata": {
        "id": "-ooYi8aGy5gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'wsj_0010_sample.txt.conll'\n",
        "output_file_path_pos = 'wsj_0010_sample.txt.pos'\n",
        "\n",
        "with open(file_path, 'r') as input_file, open(output_file_path_pos, 'w') as output_file:\n",
        "    for line in input_file:\n",
        "        if line.strip():\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) > 3:\n",
        "                token = parts[1]\n",
        "                tag = parts[3]\n",
        "                output_file.write(f'{token}\\t{tag}\\n')\n"
      ],
      "metadata": {
        "id": "gUT3omIly5u3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "64f6fa9c-d2a8-4445-a823-c2bc8c550121"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/wsj_0010_sample.txt.conll'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b3267c03e9f9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_file_path_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data/wsj_0010_sample.txt.pos'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/wsj_0010_sample.txt.conll'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Utilisation de la plateforme NLTK pour l’analyse syntaxique"
      ],
      "metadata": {
        "id": "7YVhQMJ38-VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ecrire un programme Python utilisant le package parse pour extraire les mots composés\n",
        "(chunks) ayant la structure syntaxique Déterminant-Adjectif-Nom (grammar =\n",
        "\"Compound: \\{\\<DT\\>?\\<JJ\\>*\\<NN\\>\\}\") présents dans le texte du fichier\n",
        "wsj_0010_sample.txt. Le résultat de ce module sera mis dans le fichier\n",
        "wsj_0010_sample.txt.chk.nltk.\n",
        "Note :\n",
        "Un exemple d’utilisation du package parse se trouve sur le lien\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html."
      ],
      "metadata": {
        "id": "DR7DK9JR6hpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# On s'assure d'avoir téléchargé le corpus nécessaire pour pos_tag\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "O0B7vDrH99Lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc12289-3a06-456d-d0ba-7d8cf70e6b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le contenu du fichier\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenisation du texte\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Analyse morpho-syntaxique avec pos_tag\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Création du texte avec les tags au format souhaité (Token \\t Tag)\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "# Écrire les tags dans un document temporaire\n",
        "with open('tags_temp.txt', 'w') as tags_temp_file:\n",
        "    tags_temp_file.write(tagged_text)\n",
        "\n",
        "# Définir une grammaire plus restrictive pour les chunks Déterminant-Adjectif-Nom\n",
        "grammar = r\"\"\"\n",
        "    Compound: {<DT>?<JJ>*<NN>}\n",
        "\"\"\"\n",
        "\n",
        "# Créer un analyseur de chunk basé sur la grammaire\n",
        "cp = RegexpParser(grammar)\n",
        "\n",
        "# Analyse des chunks dans le texte\n",
        "chunked = cp.parse(tagged_tokens)\n",
        "\n",
        "# Préparer le texte de sortie pour les chunks\n",
        "output_chunks = []\n",
        "\n",
        "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Compound'):\n",
        "    chunk_words = [word for word, tag in subtree.leaves()]\n",
        "\n",
        "    # Vérifier que le chunk a au moins un déterminant, un adjectif et un nom\n",
        "    if any(word[1].startswith('DT') for word in subtree.leaves()) and \\\n",
        "       any(word[1].startswith('JJ') for word in subtree.leaves()) and \\\n",
        "       any(word[1].startswith('NN') for word in subtree.leaves()):\n",
        "        output_chunks.append(' '.join(chunk_words))\n",
        "\n",
        "output_text = \"\\n\".join(output_chunks)\n",
        "\n",
        "# Écrire les chunks dans un fichier de sortie\n",
        "with open('wsj_0010_sample.txt.chk.nltk', 'w') as output_file:\n",
        "    output_file.write(output_text)\n",
        "\n",
        "print(f\"Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\")\n"
      ],
      "metadata": {
        "id": "OwWT-OFf9ASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5422a28-628f-43c0-8ff8-faccb402bad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Charger le contenu du fichier\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Charger le fichier déclaratif des structures syntaxiques\n",
        "with open('syntax_patterns.txt', 'r') as patterns_file:\n",
        "    syntax_patterns = [line.strip() for line in patterns_file]\n",
        "\n",
        "# Tokenisation du texte\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Analyse morpho-syntaxique avec pos_tag\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Préparer le texte de sortie\n",
        "output_text = \"\"\n",
        "\n",
        "for pattern in syntax_patterns:\n",
        "    # Définir la grammaire basée sur le modèle de la structure\n",
        "    grammar = f\"Compound: {pattern}\"\n",
        "\n",
        "    # Créer un analyseur de chunk basé sur la grammaire\n",
        "    cp = RegexpParser(grammar)\n",
        "\n",
        "    # Analyse des chunks dans le texte\n",
        "    chunked = cp.parse(tagged_tokens)\n",
        "\n",
        "    # Préparer le texte de sortie pour la structure\n",
        "    output_text += f\"\\n{pattern}:\\n\"\n",
        "    output_chunks = []\n",
        "\n",
        "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Compound'):\n",
        "        chunk_words = [word for word, tag in subtree.leaves()]\n",
        "        output_chunks.append(' '.join(chunk_words))\n",
        "\n",
        "    output_text += \"\\n\".join(output_chunks)\n",
        "\n",
        "# Écrire les chunks dans un fichier de sortie\n",
        "with open('wsj_0010_sample.txt.chk.nltk', 'w') as output_file:\n",
        "    output_file.write(output_text)\n",
        "\n",
        "print(f\"Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\")\n",
        "\n",
        "# Cette partie doit être améliorée, ça fait pas le bon truc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfwhAI5WBLeW",
        "outputId": "6c7b2b8f-6291-4be3-e162-4b0b04bcc5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\n"
          ]
        }
      ]
    }
  ]
}