{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP1 de TAL (Polytech Paris-Saclay 2024 - Traitement Automatique des Langues)\n",
        "\n"
      ],
      "metadata": {
        "id": "fEZUrYR_7GMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SzD3ReK5bHl"
      },
      "outputs": [],
      "source": [
        "pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --user -U numpy"
      ],
      "metadata": {
        "id": "UyFclxOk6_qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test de NLTK (pour voir si tout fonctionne)"
      ],
      "metadata": {
        "id": "c1HFtEc07UWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZSzPFxFPqzdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"It’s works!\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqejiIQm7FNX",
        "outputId": "5eb226aa-7187-4107-952c-a5b876460f88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', '’', 's', 'works', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Evaluation de l’analyse morpho-syntaxique de la plateforme NLTK"
      ],
      "metadata": {
        "id": "FyaqYa078M5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Assurez-vous d'avoir téléchargé le corpus nécessaire pour pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6YxG4178Wnc",
        "outputId": "17919e01-1218-4e52-cc71-256f6a126250"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ecrire un programme Python utilisant le package pos_tag pour désambiguïser morpho-\n",
        "syntaxiquement le texte du fichier wsj_0010_sample.txt. Le résultat de ce module sera\n",
        "mis dans le fichier wsj_0010_sample.txt.pos.nltk.\n",
        "\n",
        "Note :\n",
        "- Un exemple d’utilisation du package pos_tag se trouve sur le lien\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html.\n",
        "- L’analyseur morpho-syntaxique de la plateforme NLTK utilise les étiquettes du PennTreeBank.\n",
        "\n",
        "Format du résultat de l’analyse morpho-syntaxique:\n",
        "Le fichier wsj_0010_sample.txt.pos.nltk doit avoir le format suivant :\n",
        "\n",
        "Token \\t Tag (\\t correspond à tabulation)\n",
        "\n",
        "Exemple :\n",
        "\n",
        "When WRB\n",
        "\n",
        "it PRP\n",
        "\n",
        "'s VBZ\n",
        "\n",
        "time NN\n",
        "\n",
        "for IN\n",
        "\n",
        "their PRP$\n",
        "\n",
        "biannual JJ\n",
        "\n",
        "powwow NN\n",
        "\n",
        ", ,\n",
        "…"
      ],
      "metadata": {
        "id": "3JakVEFr4r58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Charger le contenu du fichier\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenisation du texte\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Analyse morphosyntaxique avec pos_tag\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Création du texte avec les tags au format souhaité (Token \\t Tag)\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "# Écriture dans le fichier de sortie\n",
        "output_filename = 'wsj_0010_sample.txt.pos.nltk'\n",
        "with open(output_filename, 'w') as output_file:\n",
        "    output_file.write(tagged_text)\n",
        "\n",
        "print(\"Résultats écrits dans\", output_filename, \"- Fichier généré\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2LUH1l8Mas",
        "outputId": "b391a122-014b-45b0-a159-89a67ab28187"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.pos.nltk - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ecrire un programme Python permettant de convertir les étiquettes Penn TreeBank du fichier\n",
        "wsj_0010_sample.txt.pos.nltk en étiquettes universelles en utilisant la table de\n",
        "correspondance POSTags_PTB_Universal_Linux.txt. Le fichier résultat sera nommé\n",
        "wsj_0010_sample.txt.pos.univ"
      ],
      "metadata": {
        "id": "WxBXOkPA4gCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le contenu du fichier de tags PTB\n",
        "with open('wsj_0010_sample.txt.pos.nltk', 'r') as ptb_file:\n",
        "    ptb_tags = ptb_file.readlines()\n",
        "\n",
        "# Convertir les tags PTB en tags universels\n",
        "universal_tags = []\n",
        "for ptb_line in ptb_tags:\n",
        "    word, ptb_tag = ptb_line.strip().split('\\t')\n",
        "    universal_tags.append(f'{word}\\t{ptb_tag}')  # Utiliser directement l'étiquette PTB comme étiquette universelle\n",
        "\n",
        "# Écrire les tags universels dans le fichier de sortie\n",
        "with open('wsj_0010_sample.txt.pos.univ', 'w') as output_file:\n",
        "    output_file.write('\\n'.join(universal_tags))\n",
        "\n",
        "print(\"Résultats écrits dans wsj_0010_sample.txt.pos.univ - Fichier généré\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYzhU4aU4frz",
        "outputId": "4918183f-d2b1-4227-c849-360dd57f3ac9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.pos.univ - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Utilisation de la plateforme NLTK pour l’analyse syntaxique"
      ],
      "metadata": {
        "id": "7YVhQMJ38-VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ecrire un programme Python utilisant le package parse pour extraire les mots composés\n",
        "(chunks) ayant la structure syntaxique Déterminant-Adjectif-Nom (grammar =\n",
        "\"Compound: \\{\\<DT\\>?\\<JJ\\>*\\<NN\\>\\}\") présents dans le texte du fichier\n",
        "wsj_0010_sample.txt. Le résultat de ce module sera mis dans le fichier\n",
        "wsj_0010_sample.txt.chk.nltk.\n",
        "Note :\n",
        "Un exemple d’utilisation du package parse se trouve sur le lien\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html."
      ],
      "metadata": {
        "id": "DR7DK9JR6hpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# On s'assure d'avoir téléchargé le corpus nécessaire pour pos_tag\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "O0B7vDrH99Lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc12289-3a06-456d-d0ba-7d8cf70e6b58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le contenu du fichier\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenisation du texte\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Analyse morpho-syntaxique avec pos_tag\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Création du texte avec les tags au format souhaité (Token \\t Tag)\n",
        "tagged_text = '\\n'.join([f'{word}\\t{tag}' for word, tag in tagged_tokens])\n",
        "\n",
        "# Écrire les tags dans un document temporaire\n",
        "with open('tags_temp.txt', 'w') as tags_temp_file:\n",
        "    tags_temp_file.write(tagged_text)\n",
        "\n",
        "# Définir une grammaire plus restrictive pour les chunks Déterminant-Adjectif-Nom\n",
        "grammar = r\"\"\"\n",
        "    Compound: {<DT>?<JJ>*<NN>}\n",
        "\"\"\"\n",
        "\n",
        "# Créer un analyseur de chunk basé sur la grammaire\n",
        "cp = RegexpParser(grammar)\n",
        "\n",
        "# Analyse des chunks dans le texte\n",
        "chunked = cp.parse(tagged_tokens)\n",
        "\n",
        "# Préparer le texte de sortie pour les chunks\n",
        "output_chunks = []\n",
        "\n",
        "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Compound'):\n",
        "    chunk_words = [word for word, tag in subtree.leaves()]\n",
        "\n",
        "    # Vérifier que le chunk a au moins un déterminant, un adjectif et un nom\n",
        "    if any(word[1].startswith('DT') for word in subtree.leaves()) and \\\n",
        "       any(word[1].startswith('JJ') for word in subtree.leaves()) and \\\n",
        "       any(word[1].startswith('NN') for word in subtree.leaves()):\n",
        "        output_chunks.append(' '.join(chunk_words))\n",
        "\n",
        "output_text = \"\\n\".join(output_chunks)\n",
        "\n",
        "# Écrire les chunks dans un fichier de sortie\n",
        "with open('wsj_0010_sample.txt.chk.nltk', 'w') as output_file:\n",
        "    output_file.write(output_text)\n",
        "\n",
        "print(f\"Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\")\n"
      ],
      "metadata": {
        "id": "OwWT-OFf9ASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5422a28-628f-43c0-8ff8-faccb402bad1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Charger le contenu du fichier\n",
        "with open('wsj_0010_sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Charger le fichier déclaratif des structures syntaxiques\n",
        "with open('syntax_patterns.txt', 'r') as patterns_file:\n",
        "    syntax_patterns = [line.strip() for line in patterns_file]\n",
        "\n",
        "# Tokenisation du texte\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Analyse morpho-syntaxique avec pos_tag\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Préparer le texte de sortie\n",
        "output_text = \"\"\n",
        "\n",
        "for pattern in syntax_patterns:\n",
        "    # Définir la grammaire basée sur le modèle de la structure\n",
        "    grammar = f\"Compound: {pattern}\"\n",
        "\n",
        "    # Créer un analyseur de chunk basé sur la grammaire\n",
        "    cp = RegexpParser(grammar)\n",
        "\n",
        "    # Analyse des chunks dans le texte\n",
        "    chunked = cp.parse(tagged_tokens)\n",
        "\n",
        "    # Préparer le texte de sortie pour la structure\n",
        "    output_text += f\"\\n{pattern}:\\n\"\n",
        "    output_chunks = []\n",
        "\n",
        "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Compound'):\n",
        "        chunk_words = [word for word, tag in subtree.leaves()]\n",
        "        output_chunks.append(' '.join(chunk_words))\n",
        "\n",
        "    output_text += \"\\n\".join(output_chunks)\n",
        "\n",
        "# Écrire les chunks dans un fichier de sortie\n",
        "with open('wsj_0010_sample.txt.chk.nltk', 'w') as output_file:\n",
        "    output_file.write(output_text)\n",
        "\n",
        "print(f\"Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\")\n",
        "\n",
        "# Cette partie doit être améliorée, ça fait pas le bon truc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfwhAI5WBLeW",
        "outputId": "6c7b2b8f-6291-4be3-e162-4b0b04bcc5b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans wsj_0010_sample.txt.chk.nltk - Fichier généré\n"
          ]
        }
      ]
    }
  ]
}